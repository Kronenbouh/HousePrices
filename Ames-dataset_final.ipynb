{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House price prediction\n",
    "\n",
    "This notebook his my personal take on the kaggle competition \"House Prices: Advanced Regression Techniques\" (see: https://www.kaggle.com/c/house-prices-advanced-regression-techniques).\n",
    "\n",
    "I would like to give credit to the following Kaggle users who helped me by sharing their work:\n",
    "\n",
    "- Goldens https://www.kaggle.com/goldens/house-prices-on-the-top-with-a-simple-model\n",
    "- Pedro Marcelino https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n",
    "\n",
    "and more generally, the Kaggle community by sharing their thoughts and advice on the forum.\n",
    "\n",
    "## My approach to this problem\n",
    "\n",
    "My approach is pretty straight-forward, a little bit of data cleansing and then modelling with the off-the-shelf models I know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The packages used in this notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV, SGDRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Two personal functions used in the notebook\n",
    "\n",
    "def mapToOrd(df, cols, mapping):\n",
    "    return(df[cols].apply(lambda x: x.map(mapping)))\n",
    "\n",
    "def rootMSE(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    return(np.sqrt(((y_pred-y_true)**2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "This part consist mostly in:\n",
    "\n",
    "- finding an deleting outliers;\n",
    "- finding and replacing missing values;\n",
    "- transforming highly skewed data to normalize them;\n",
    "- using one-hot encoding for categorical data;\n",
    "- Scale data.\n",
    "\n",
    "When replacing missing values, we will keep the training and testing data set seprated to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data, keeping training and testing separated\n",
    "\n",
    "train = pd.read_csv('train.csv', header = 0)\n",
    "test = pd.read_csv('test.csv', header = 0)\n",
    "train.drop('Id', axis=1, inplace=True)\n",
    "test.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "# Setting the dependant variable\n",
    "target = 'SalePrice'\n",
    "\n",
    "# MSSubClass and YrSold are encoded as integer-valued varaibles but they are categorical variables\n",
    "cols = ['MSSubClass', 'YrSold']\n",
    "train[cols] = train[cols].astype('object')\n",
    "test[cols] = test[cols].astype('object')\n",
    "\n",
    "# Find categorial and numerical values\n",
    "categorical = train.select_dtypes(include=['object']).columns\n",
    "numerical = train.select_dtypes(include=['int64','float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1014f760>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Below chart will highlight some outliers for the variable GrLivArea in the training data\n",
    "\n",
    "plt.scatter(train['GrLivArea'], train[target])\n",
    "plt.plot([4600, 4600], [0, 900000], 'k-', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train \n",
      " LotFrontage    259\n",
      "MasVnrArea       8\n",
      "GarageYrBlt     81\n",
      "dtype: int64 \n",
      " For test \n",
      " LotFrontage     227\n",
      "MasVnrArea       15\n",
      "BsmtFinSF1        1\n",
      "BsmtFinSF2        1\n",
      "BsmtUnfSF         1\n",
      "TotalBsmtSF       1\n",
      "BsmtFullBath      2\n",
      "BsmtHalfBath      2\n",
      "GarageYrBlt      78\n",
      "GarageCars        1\n",
      "GarageArea        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop outliers for 'GrLivArea'\n",
    "\n",
    "train = train.drop(train[train['GrLivArea']>4600].index)\n",
    "\n",
    "# Inspect missing numerical values\n",
    "\n",
    "print('For train \\n', train[numerical].isnull().sum()[train[numerical].isnull().sum()>0],\n",
    "      '\\n For test \\n', test[numerical.drop(target)].isnull().sum()[test[numerical.drop(target)].isnull().sum()>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train \n",
      " Alley           1367\n",
      "MasVnrType         8\n",
      "BsmtQual          37\n",
      "BsmtCond          37\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtFinType2      38\n",
      "Electrical         1\n",
      "FireplaceQu      690\n",
      "GarageType        81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "PoolQC          1452\n",
      "Fence           1177\n",
      "MiscFeature     1404\n",
      "dtype: int64 \n",
      " For test \n",
      " MSZoning           4\n",
      "Alley           1352\n",
      "Utilities          2\n",
      "Exterior1st        1\n",
      "Exterior2nd        1\n",
      "MasVnrType        16\n",
      "BsmtQual          44\n",
      "BsmtCond          45\n",
      "BsmtExposure      44\n",
      "BsmtFinType1      42\n",
      "BsmtFinType2      42\n",
      "KitchenQual        1\n",
      "Functional         2\n",
      "FireplaceQu      730\n",
      "GarageType        76\n",
      "GarageFinish      78\n",
      "GarageQual        78\n",
      "GarageCond        78\n",
      "PoolQC          1456\n",
      "Fence           1169\n",
      "MiscFeature     1408\n",
      "SaleType           1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# For some variables, NA indicates the absence of the variable (e.g. no garage), those will be replaced by 0\n",
    "numto0 = ['GarageYrBlt', 'GarageCars', 'GarageArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n",
    "         'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\n",
    "\n",
    "train[numto0] = train[numto0].fillna(value=0)\n",
    "test[numto0] = test[numto0].fillna(value=0)\n",
    "\n",
    "# For lot frontage we will replace by the mean\n",
    "numtoMean = ['LotFrontage']\n",
    "\n",
    "train[numtoMean] = train[numtoMean].fillna(value=train[numtoMean].mean())\n",
    "test[numtoMean] = test[numtoMean].fillna(value=test[numtoMean].mean())\n",
    "\n",
    "#Inspect missing categorial values\n",
    "\n",
    "print('For train \\n', train[categorical].isnull().sum()[train[categorical].isnull().sum()>0],\n",
    "      '\\n For test \\n', test[categorical].isnull().sum()[test[categorical].isnull().sum()>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train \n",
      " Series([], dtype: int64) \n",
      " For test \n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# For some variables, NA indicates the absence of the variable (e.g. no garage), those will be replaced by None\n",
    "catwNA = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical',\n",
    "          'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'Functional']\n",
    "\n",
    "train[catwNA] = train[catwNA].fillna(value='None')\n",
    "test[catwNA] = test[catwNA].fillna(value='None')\n",
    "\n",
    "# For the other we will replace NA by the most frequent value (the mode), this only concern the test set\n",
    "catwoNA = list(set(test[categorical].isnull().sum()[test[categorical].isnull().sum()>0].index)-set(catwNA))\n",
    "\n",
    "test[catwoNA] = test[catwoNA].apply(lambda x: x.fillna(value=x.mode()[0]))\n",
    "\n",
    "# Sanity check, two empty series are expected\n",
    "\n",
    "print('For train \\n', train.isnull().sum()[train.isnull().sum()>0],\n",
    "      '\\n For test \\n', test.isnull().sum()[test.isnull().sum()>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting ordinal data (optional)\n",
    "\n",
    "Ordinal data are encoded as categorical. To preserve the order of the data we can convert the labels to integer. Since doing this do not gives good result when testing the models, this part of the notebook is entirely commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order1 = {'Ex' : 4, 'Gd' : 3, 'TA' : 2, 'Fa' : 1, 'Po' : 1, 'None': 0}\n",
    "# catToOrd1 = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', \n",
    "#              'GarageCond', 'PoolQC']\n",
    "\n",
    "# order2 = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n",
    "# catToOrd2 = ['BsmtExposure']\n",
    "\n",
    "# order3 = {'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'None':0}\n",
    "# catToOrd3 = ['BsmtFinType1', 'BsmtFinType2']\n",
    "\n",
    "# order4 = {'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, 'Maj2': 3, 'Sev': 2, 'Sal': 1, 'None': 0}\n",
    "# catToOrd4 = ['Functional']\n",
    "\n",
    "# order5 = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'None': 0}\n",
    "# catToOrd5 = ['GarageFinish']\n",
    "\n",
    "# order6 = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'None': 0}\n",
    "# catToOrd6 = ['Fence']\n",
    "\n",
    "# for i in range(1,7):\n",
    "#     train[globals()['catToOrd'+str(i)]] = mapToOrd(train, globals()['catToOrd'+str(i)], globals()['order'+str(i)])\n",
    "#     test[globals()['catToOrd'+str(i)]] = mapToOrd(test, globals()['catToOrd'+str(i)], globals()['order'+str(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation\n",
    "\n",
    "Now that missing values are replaced we need to:\n",
    "\n",
    "- transform skewed data (log-transformation);\n",
    "- apply one-hot encoding (a.k.a. dummy encoding).\n",
    "\n",
    "For the encoding, training and testing data sets must be concatenated in one dataframe. This concatenation will also be convenient for the log-transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y for future model training\n",
    "X = train.drop(target, axis=1)\n",
    "y = train[target]\n",
    "\n",
    "# the len of the training data set must be stored to de-concatenate\n",
    "len_X = X.shape[0]\n",
    "\n",
    "# Concatenation\n",
    "data = pd.concat([X,test], sort=False)\n",
    "\n",
    "# Find highly skewed features and apply log transformation\n",
    "skewed_feats = data[numerical.drop(target)].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.8]\n",
    "skewed_feats = skewed_feats.index\n",
    "data[skewed_feats] = np.log1p(data[skewed_feats])\n",
    "\n",
    "# Apply log-transformation to the target\n",
    "y = np.log1p(y)\n",
    "\n",
    "# Get dummies\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# Retrieve train and test\n",
    "X = data[:len_X]\n",
    "test = data[len_X:]\n",
    "\n",
    "# Scale data\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split on the training dataset\n",
    "\n",
    "In order to test the generalization ability of our model, we use the standard train_test_split function to keep a portion of the training data (i.e. the data for which the sale price is known) as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models\n",
    "\n",
    "In this part we will train and compare several models on the test data set. The cost function is the root mean log squarred error (which is the root mean square error between the log prediction and the log true price). Since we used a log transformation for the target variable this metric is very convenient (considering that we only manipulate log-values we will refer to the root mean log squared error as \"mean rmse\").\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "When relevant, we will use cross-validation to tune the hyperparameter of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha chosen :  0.0005\n",
      "Mean rmse training for chosen alpha:  0.08897986271017874\n",
      "Mean rmse testing:  0.10720333225837815\n"
     ]
    }
   ],
   "source": [
    "# First, Lasso regression\n",
    "\n",
    "lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005, 0.0001], max_iter=10000, cv=10).fit(X_train, y_train)\n",
    "\n",
    "print('alpha chosen: ', lasso.alpha_)\n",
    "print('rmse training training for chosen alpha: ', \n",
    "      np.sqrt(-cross_val_score(lasso, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 5)).min())\n",
    "print('rmse testing: ', np.sqrt(mean_squared_error(lasso.predict(X_test), y_test)))\n",
    "\n",
    "# The lasso regression perform nicely with good generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha chosen :  1.0\n",
      "Mean rmse training for chosen alpha:  0.10607143225259545\n",
      "Mean rmse score testing :  0.11755501159922621\n"
     ]
    }
   ],
   "source": [
    "#Second, Ridge regression\n",
    "\n",
    "ridge = RidgeCV(alphas = [1, 0.1, 0.001, 0.0005, 0.0001], cv=10).fit(X_train, y_train)\n",
    "\n",
    "print('alpha chosen : ', ridge.alpha_)\n",
    "print('rmse training for chosen alpha: ', np.sqrt(-cross_val_score(ridge, X, y, scoring=\"neg_mean_squared_error\", cv = 5)).min())\n",
    "print('rmse testing: ', np.sqrt(mean_squared_error(ridge.predict(X_test), y_test)))\n",
    "\n",
    " # The Ridge regression is also suitable but a little bit less than the lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min samples leaf chose:  {'min_samples_leaf': 20}\n",
      "Mean rmse training:  [0.14093463 0.14433357 0.15248714 0.16799692]\n",
      "Mean rmse testing:  0.15948276654275553\n"
     ]
    }
   ],
   "source": [
    "# Third, Random forest regression\n",
    "\n",
    "RFR = RandomForestRegressor()\n",
    "parameters = {'min_samples_leaf': [1, 5, 10, 20]}\n",
    "gridRFR = GridSearchCV(RFR, parameters, scoring = rootMSE, cv = 5).fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter: ', gridRFR.best_params_)\n",
    "print('rmse training: ', gridRFR.cv_results_['mean_test_score'])\n",
    "print('rmse testing: ', np.sqrt(mean_squared_error(gridRFR.predict(X_test), y_test)))\n",
    "\n",
    "# Random forest regression performs poorly compared to lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rmse :  [0.12567065 0.12402143 0.12754673 0.12962721]\n",
      "Mean rmse testing :  0.1264489787199797\n"
     ]
    }
   ],
   "source": [
    "# Fourth, gradient boosting\n",
    "\n",
    "parameters = {'min_samples_leaf': [1, 5, 10, 20]}\n",
    "\n",
    "GBR = GradientBoostingRegressor(n_estimators=100, max_depth=3)\n",
    "gridGBR = GridSearchCV(GBR, parameters, scoring = rootMSE, cv = 5).fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter: ', gridGBR.best_params_)\n",
    "print('rmse training for chosen paramater: ', gridGBR.cv_results_['mean_test_score'])\n",
    "print('Mean rmse testing : ', np.sqrt(mean_squared_error(gridGBR.predict(X_test), y_test)))\n",
    "\n",
    "# Gradient boosting performance are average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean rmse :  [0.13421237 0.12958431 0.11973102 0.12858176 0.14165405]\n",
      "Mean rmse testing :  0.12598714976796632\n"
     ]
    }
   ],
   "source": [
    "# Fifth support vector regressor\n",
    "\n",
    "parameters = {'C': [1, 0.5, 0.1, 0.001, 0.0005]}\n",
    "\n",
    "SVregr = SVR(kernel='linear')\n",
    "gridSVR = GridSearchCV(SVregr, parameters, scoring = rootMSE, cv = 5).fit(X_train, y_train)\n",
    "\n",
    "print('Best parameter: ', gridSVR.best_params_)\n",
    "print('rmse training for chosen parameter: ', gridSVR.cv_results_['mean_test_score'])\n",
    "print('rmse testing : ', np.sqrt(mean_squared_error(gridSVR.predict(X_test), y_test)))\n",
    "\n",
    "# SVR performance are average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean rmse training :  0.08947037410868156\n",
      "alpha chosen :  0.0005\n",
      "l1 chosen :  0.7\n",
      "Mean rmse testing :  0.10773692014910224\n"
     ]
    }
   ],
   "source": [
    "# Sixth, elasctinet\n",
    "\n",
    "elasti = ElasticNetCV(l1_ratio = [.1, .5, .7, .9, .95, .99, 1] ,alphas = [1, 0.1, 0.001, 0.0005, 0.0001], \n",
    "                max_iter=10000, cv=5).fit(X_train, y_train)\n",
    "\n",
    "print('alpha chosen: ', elasti.alpha_)\n",
    "print('l1 chosen: ', elasti.l1_ratio_)\n",
    "print('rmse training for chosen paramaters: ', np.sqrt(-cross_val_score(elasti, X_train, y_train, \n",
    "                                                   scoring=\"neg_mean_squared_error\", cv = 5)).min())\n",
    "print('rmse testing: ', np.sqrt(mean_squared_error(elasti.predict(X_test), y_test)))\n",
    "\n",
    "# Elasticnet model offers performance comparable to a Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse training :  [0.20704412 0.40588016 0.30128559 0.41418056]\n",
      "Mean rmse testing :  0.32841281596205507\n"
     ]
    }
   ],
   "source": [
    "# Seventh, MLP\n",
    "\n",
    "grid_act = {'activation': ['identity', 'logistic', 'tanh', 'relu']}\n",
    "\n",
    "MLP = MLPRegressor(hidden_layer_sizes=(100,50,50,50))\n",
    "\n",
    "grid_MLP = GridSearchCV(MLP, grid_act, scoring = rootMSE, cv = 5).fit(X_train, y_train)\n",
    "\n",
    "print('rmse training: ', grid_MLP.cv_results_['mean_test_score'])\n",
    "print('Mean rmse testing: ', np.sqrt(mean_squared_error(grid_MLP.predict(X_test), y_test)))\n",
    "\n",
    "# Multi-layer perceptron performs quite poorly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking regressor\n",
    "\n",
    "All the models above (except the MLP which performs so poorly) will be stacked in one big regressor. The choice of the final (or meta) estimator will be done by comparing several results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigth, stacked\n",
    "\n",
    "estimators = [('ridge', Ridge(alpha=1., max_iter=10000)), ('lasso', Lasso(alpha=0.0005, max_iter=100000)),\n",
    "             ('elasti', ElasticNet(l1_ratio = 0.7, alpha=0.0005, max_iter=100000))]\n",
    "\n",
    "# The code below was used to compare several meta-estimator with several paramters. \n",
    "# Since it takes quite long to compute, it has been commented.\n",
    "\n",
    "# for final_estimator in ['Ridge', 'Lasso']:\n",
    "#     print(final_estimator)\n",
    "#     for alpha in [1, 0.1, 0.01, 0.001, 0.0005, 0.0001]:\n",
    "#         if(final_est == 'Ridge'):\n",
    "#             reg = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha = alpha)).fit(X_train, y_train)\n",
    "#             print('Mean rmse score testing for ', alpha,' = ', np.sqrt(mean_squared_error(reg.predict(X_test), y_test)))\n",
    "#         else:\n",
    "#             reg = StackingRegressor(estimators=estimators, final_estimator=Lasso(alpha = alpha)).fit(X_train, y_train)\n",
    "#             print('Mean rmse score testing for ', alpha,' = ', np.sqrt(mean_squared_error(reg.predict(X_test), y_test)))\n",
    "# double loop above give the best rmse testing for lasso with alpha = 0.001\n",
    "\n",
    "# for alpha in [1, 0.1, 0.01, 0.001, 0.0005, 0.0001]:\n",
    "#     for l1_ratio in [.1, .5, .7, .9, .95, .99, 1]:\n",
    "#         reg = StackingRegressor(estimators=estimators, \n",
    "#                                 final_estimator=ElasticNet(alpha = alpha, l1_ratio = l1_ratio)).fit(X_train, y_train)\n",
    "#         print('Mean rmse score testing for ', alpha, ' ', l1_ratio, ' = ', \n",
    "#               np.sqrt(mean_squared_error(reg.predict(X_test), y_test)))\n",
    "\n",
    "# double loop above give the best rmse testing for alpha = 0.0005 l1_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse score testing for Lasso:  0.10768773658560624\n",
      "rmse score testing for Elastic: 0.1079094748994152\n"
     ]
    }
   ],
   "source": [
    "# Finally we compare two choices of meta-estimator with the best estimation for the hyper-paramaters.\n",
    "\n",
    "reg1 = StackingRegressor(estimators=estimators, final_estimator=Lasso(alpha = 0.001, max_iter=100000)).fit(X_train, y_train)\n",
    "reg2 = StackingRegressor(estimators=estimators, \n",
    "                         final_estimator=ElasticNet(alpha = 0.0005, l1_ratio = 0.9, max_iter=100000)).fit(X_train, y_train)\n",
    "print('rmse testing for Lasso: ', np.sqrt(mean_squared_error(reg1.predict(X_test), y_test)))\n",
    "print('rmse testing for Elastic:', np.sqrt(mean_squared_error(reg2.predict(X_test), y_test)))\n",
    "\n",
    "# Lasso is found to perform slighly better. Nonetheless, stacking regressor do not improve the performance on the test set.\n",
    "# The pertinence of using such a technique is therefore disputed for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final estimation\n",
    "\n",
    "Now that we found the best pick for the final estimator we will just train it on the full training data.\n",
    "We should not forget to inverse the transformation applied on the sale price, i.e. exponentiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha chosen :  0.0005\n",
      "rmse for alpha chosen:  0.10217841713497391\n"
     ]
    }
   ],
   "source": [
    "# Model 1 : lasso\n",
    "\n",
    "model = LassoCV(alphas = [1, 0.1, 0.001, 0.0005, 0.0001], max_iter=10000, cv=10).fit(X, y)\n",
    "\n",
    "print('alpha chosen : ', model.alpha_) \n",
    "print('rmse for alpha chosen: ', np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = 5)).min())\n",
    "\n",
    "# Prediction with the model\n",
    "res = np.expm1(model.predict(test))\n",
    "pd.DataFrame(res, index=list(range(1461,2920)), columns=['SalePrice']).to_csv(\"mypred.csv\", index_label='Id')\n",
    "\n",
    "# The chosen alpha is consistent with the one found during testing.\n",
    "# Those predictions scored 0.11904, in the top 21% as of 07/02/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean mse score :  0.09161238782968201\n"
     ]
    }
   ],
   "source": [
    "# Model 2 : stacked regressors\n",
    "\n",
    "estimators = [('ridge', Ridge(alpha=1.)), ('lasso', Lasso(alpha=0.0005, max_iter=10000)), ('forest', RandomForestRegressor()),\n",
    "             ('gradient', GradientBoostingRegressor(min_samples_leaf=10)), ('SVR', SVR(kernel='linear', C=0.1)),\n",
    "             ('elasti', ElasticNet(l1_ratio = 0.7, alpha=0.0005, max_iter=10000))]\n",
    "\n",
    "stack = StackingRegressor(estimators=estimators, \n",
    "                         final_estimator=Lasso(alpha = 0.001, max_iter=100000)).fit(X, y)\n",
    "\n",
    "print('Mean mse score : ', np.sqrt(mean_squared_error(stack.predict(X), y)))\n",
    "\n",
    "res = np.expm1(stack.predict(test))\n",
    "pd.DataFrame(res, index=list(range(1461,2920)), columns=['SalePrice']).to_csv(\"mypred_Stacked.csv\", index_label='Id')\n",
    "\n",
    "# Those predictions scored 0.11860, in the top 20% as of 07/02/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By stacking regressors we were able to reach a decent rmse on 50% of the log-price of the test set. \n",
    "We still do not explain why stacking performance were lower than simple lasso while testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
